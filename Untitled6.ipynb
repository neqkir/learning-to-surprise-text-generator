{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTN6rGEwL3xo",
        "outputId": "5bfcafd6-00fd-4ea0-de09-55a1e91f9410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded training data...\n",
            "Length of text: 10210 characters\n",
            "4070 unique characters\n"
          ]
        }
      ],
      "source": [
        "## audience-composer model \n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Training the composer on Mallarme'\n",
        "training_file = 'mallarme.txt'\n",
        "\n",
        "def read_data(fname):\n",
        "    with open(fname) as f:\n",
        "        content = f.readlines()\n",
        "    content = [x.strip() for x in content]\n",
        "    content = [word for i in range(len(content)) for word in content[i].split()]\n",
        "    content = np.array(content)\n",
        "    return content\n",
        "\n",
        "mallarme = read_data(training_file)\n",
        "print(\"Loaded training data...\")\n",
        "\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(mallarme)} characters')\n",
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(mallarme))\n",
        "print(f'{len(vocab)} unique characters')\n",
        "\n",
        "# encoding Bible into integers\n",
        "ids_from_chars = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(mallarme, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "# defining the reverse operation\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "# defining the batches\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(mallarme)//(seq_length+1)\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# For training you'll need a dataset of (input, label) pairs.\n",
        "# Where input and label are sequences.\n",
        "# At each time step the input is the current character and the label is the next character.\n",
        "# Here's a function that takes a sequence as input, duplicates, and shifts it to align the input\n",
        "# and label for each timestep:\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUTk2o3eMzgi"
      },
      "outputs": [],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "len_data=len(list(dataset))\n",
        "# build training & validation datasets\n",
        "VAL_FRAC=0.2\n",
        "validation_dataset = dataset.take(int(len_data*VAL_FRAC))\n",
        "train_dataset = dataset.skip(int(len_data*VAL_FRAC))\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   reset_after=True,\n",
        "                                   activation='tanh'\n",
        "                                   )\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "    # unpack the data\n",
        "    inputs, labels = inputs\n",
        "  \n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True) # forward pass\n",
        "      # Compute the loss value\n",
        "      # (the loss function is configured in `compile()`)\n",
        "      loss=self.compiled_loss(labels, predictions, regularization_losses=self.losses)\n",
        "\n",
        "    # compute the gradients\n",
        "    grads=tape.gradient(loss, model.trainable_variables)\n",
        "    # Update weights\n",
        "    self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    # Update metrics (includes the metric that tracks the loss)\n",
        "    self.compiled_metrics.update_state(labels, predictions)\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {m.name: m.result() for m in self.metrics}\n",
        "    \n",
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units\n",
        "    )\n",
        "\n",
        "# checking the shape of the output\n",
        "for input_example_batch, target_example_batch in train_dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# checking the mean_loss\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)\n",
        "\n",
        "# A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes.\n",
        "# To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size.\n",
        "# A much higher loss means the model is sure of its wrong answers, and is badly initialized:\n",
        "tf.exp(mean_loss).numpy()\n",
        "\n",
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[\n",
        "                  tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "              )\n",
        "\n",
        "# setting early-stopping\n",
        "EarlyS = EarlyStopping(monitor = 'val_loss', mode = 'min', restore_best_weights=True, patience=10, verbose = 1)\n",
        "\n",
        "EPOCHS=30\n",
        "########## FIT\n",
        "\n",
        "history = model.fit(train_dataset, validation_data=validation_dataset, epochs=EPOCHS, callbacks = [EarlyS], verbose=1)\n",
        "\n",
        "# generate some data - the generator model\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "\n"
      ],
      "metadata": {
        "id": "5uMSDGgxNcZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['In'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n",
        "\n",
        "with open('out_bible_001.txt','a') as f:\n",
        "  f.write(result[0].numpy().decode('utf-8') + '\\n\\n' + '_'*80)\n",
        "  f.write('\\nRun time:%f'  %(end - start))\n",
        "\n",
        "# If you want the model to generate text faster\n",
        "# the easiest thing you can do is batch the text generation. In the example below the model\n",
        "# generates 5 outputs in about the same time it took to generate 1 above.\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['In', 'In', 'In', 'In', 'In'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "with open('out_bible_001_2.txt','a') as f:\n",
        "  \n",
        "  for i in range(0,4):\n",
        "    f.write(result[0].numpy().decode('utf-8') + '\\n\\n' + '_'*80)\n",
        "    print(result[i].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "  f.write('\\nRun time:%f' % (end - start))\n",
        "\n",
        "print('\\nRun time:%f' % (end - start))\n",
        "\n",
        "# save generator model\n",
        "\n",
        "if not os.path.exists('saved_model'):\n",
        "    os.makedirs('saved_model')\n",
        "    \n",
        "tf.saved_model.save(one_step_model, 'saved_model/one_step_gen')\n",
        "\n",
        "# print metrics\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(history.history.keys())\n",
        "\n",
        "acc=history.history['sparse_categorical_accuracy']\n",
        "val_acc=history.history['val_sparse_categorical_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(1, len(acc)+1)\n",
        "\n",
        "plt.plot(epochs,acc,'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.savefig('Accuracy.pdf')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs,loss,'bo',label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.savefig('Loss.pdf')"
      ],
      "metadata": {
        "id": "Lj46pf_vNRbg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}